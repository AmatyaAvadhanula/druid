diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/common/actions/SegmentTransactionalAppendAction.java b/indexing-service/src/main/java/org/apache/druid/indexing/common/actions/SegmentTransactionalAppendAction.java
index e0d155408d..95f3a96aca 100644
--- a/indexing-service/src/main/java/org/apache/druid/indexing/common/actions/SegmentTransactionalAppendAction.java
+++ b/indexing-service/src/main/java/org/apache/druid/indexing/common/actions/SegmentTransactionalAppendAction.java
@@ -23,7 +23,6 @@ import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.google.common.collect.ImmutableSet;
-import org.apache.druid.indexing.common.TaskLock;
 import org.apache.druid.indexing.common.task.IndexTaskUtils;
 import org.apache.druid.indexing.common.task.Task;
 import org.apache.druid.indexing.overlord.CriticalAction;
@@ -31,14 +30,10 @@ import org.apache.druid.indexing.overlord.DataSourceMetadata;
 import org.apache.druid.indexing.overlord.SegmentPublishResult;
 import org.apache.druid.indexing.overlord.TaskLockInfo;
 import org.apache.druid.java.util.emitter.service.ServiceMetricEvent;
-import org.apache.druid.query.DruidMetrics;
 import org.apache.druid.segment.SegmentUtils;
 import org.apache.druid.timeline.DataSegment;
-import org.joda.time.Interval;
 
 import javax.annotation.Nullable;
-import java.util.HashMap;
-import java.util.HashSet;
 import java.util.Map;
 import java.util.Set;
 import java.util.stream.Collectors;
@@ -111,42 +106,22 @@ public class SegmentTransactionalAppendAction implements TaskAction<SegmentPubli
   @Override
   public SegmentPublishResult perform(Task task, TaskActionToolbox toolbox)
   {
-    final SegmentPublishResult retVal;
-
-    final Set<DataSegment> allSegments = new HashSet<>(segments);
-
-    String datasource = task.getDataSource();
-    Map<Interval, TaskLock> replaceLocks = new HashMap<>();
-    for (TaskLock lock : TaskLocks.findReplaceLocksForSegments(datasource, toolbox.getTaskLockbox(), segments)) {
-      replaceLocks.put(lock.getInterval(), lock);
-    }
-    Map<DataSegment, TaskLockInfo> appendSegmentLockMap = new HashMap<>();
-    Set<TaskLockInfo> taskLockInfos = new HashSet<>();
-    for (TaskLock taskLock : replaceLocks.values()) {
-      taskLockInfos.add(getTaskLockInfo(taskLock));
-    }
-
-    for (DataSegment segment : segments) {
-      Interval interval = segment.getInterval();
-      for (Interval key : replaceLocks.keySet()) {
-        if (key.contains(interval)) {
-          appendSegmentLockMap.put(segment, getTaskLockInfo(replaceLocks.get(key)));
-        }
-      }
-    }
+    final String datasource = task.getDataSource();
+    final Map<DataSegment, TaskLockInfo> segmentToReplaceLock
+        = TaskLocks.findReplaceLocksCoveringSegments(datasource, toolbox.getTaskLockbox(), segments);
 
+    final SegmentPublishResult retVal;
     try {
       retVal = toolbox.getTaskLockbox().doInCriticalSection(
           task,
-          allSegments.stream().map(DataSegment::getInterval).collect(Collectors.toSet()),
+          segments.stream().map(DataSegment::getInterval).collect(Collectors.toSet()),
           CriticalAction.<SegmentPublishResult>builder()
               .onValidLocks(
                   () -> toolbox.getIndexerMetadataStorageCoordinator().commitAppendSegments(
                       segments,
                       startMetadata,
                       endMetadata,
-                      appendSegmentLockMap,
-                      taskLockInfos
+                      segmentToReplaceLock
                   )
               )
               .onInvalidLocks(
@@ -168,29 +143,17 @@ public class SegmentTransactionalAppendAction implements TaskAction<SegmentPubli
 
     if (retVal.isSuccess()) {
       toolbox.getEmitter().emit(metricBuilder.build("segment/txn/success", 1));
+      for (DataSegment segment : retVal.getSegments()) {
+        IndexTaskUtils.setSegmentDimensions(metricBuilder, segment);
+        toolbox.getEmitter().emit(metricBuilder.build("segment/added/bytes", segment.getSize()));
+      }
     } else {
       toolbox.getEmitter().emit(metricBuilder.build("segment/txn/failure", 1));
     }
 
-    // getSegments() should return an empty set if announceHistoricalSegments() failed
-    for (DataSegment segment : retVal.getSegments()) {
-      metricBuilder.setDimension(DruidMetrics.INTERVAL, segment.getInterval().toString());
-      metricBuilder.setDimension(
-          DruidMetrics.PARTITIONING_TYPE,
-          segment.getShardSpec() == null ? null : segment.getShardSpec().getType()
-      );
-      toolbox.getEmitter().emit(metricBuilder.build("segment/added/bytes", segment.getSize()));
-    }
-
     return retVal;
   }
 
-
-  private TaskLockInfo getTaskLockInfo(TaskLock taskLock)
-  {
-    return new TaskLockInfo(taskLock.getInterval(), taskLock.getVersion());
-  }
-
   @Override
   public boolean isAudited()
   {
diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/common/actions/SegmentTransactionalInsertAction.java b/indexing-service/src/main/java/org/apache/druid/indexing/common/actions/SegmentTransactionalInsertAction.java
index 5acc048fdf..270154ed0b 100644
--- a/indexing-service/src/main/java/org/apache/druid/indexing/common/actions/SegmentTransactionalInsertAction.java
+++ b/indexing-service/src/main/java/org/apache/druid/indexing/common/actions/SegmentTransactionalInsertAction.java
@@ -222,7 +222,7 @@ public class SegmentTransactionalInsertAction implements TaskAction<SegmentPubli
           allSegments.stream().map(DataSegment::getInterval).collect(Collectors.toSet()),
           CriticalAction.<SegmentPublishResult>builder()
               .onValidLocks(
-                  () -> toolbox.getIndexerMetadataStorageCoordinator().commitSegments(
+                  () -> toolbox.getIndexerMetadataStorageCoordinator().commitSegmentsAndMetadata(
                       segments,
                       segmentsToBeDropped,
                       startMetadata,
diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/common/actions/SegmentTransactionalReplaceAction.java b/indexing-service/src/main/java/org/apache/druid/indexing/common/actions/SegmentTransactionalReplaceAction.java
index 4dd3ea7a38..255628e262 100644
--- a/indexing-service/src/main/java/org/apache/druid/indexing/common/actions/SegmentTransactionalReplaceAction.java
+++ b/indexing-service/src/main/java/org/apache/druid/indexing/common/actions/SegmentTransactionalReplaceAction.java
@@ -23,7 +23,6 @@ import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.google.common.collect.ImmutableSet;
-import org.apache.druid.indexing.common.TaskLock;
 import org.apache.druid.indexing.common.task.IndexTaskUtils;
 import org.apache.druid.indexing.common.task.Task;
 import org.apache.druid.indexing.overlord.CriticalAction;
@@ -33,10 +32,8 @@ import org.apache.druid.java.util.emitter.service.ServiceMetricEvent;
 import org.apache.druid.query.DruidMetrics;
 import org.apache.druid.segment.SegmentUtils;
 import org.apache.druid.timeline.DataSegment;
-import org.joda.time.Interval;
 
 import javax.annotation.Nullable;
-import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Map;
 import java.util.Set;
@@ -119,32 +116,21 @@ public class SegmentTransactionalReplaceAction implements TaskAction<SegmentPubl
   @Override
   public SegmentPublishResult perform(Task task, TaskActionToolbox toolbox)
   {
-    final SegmentPublishResult retVal;
-
-    final Set<DataSegment> allSegments = new HashSet<>(segments);
-
-    TaskLocks.checkLockCoversSegments(task, toolbox.getTaskLockbox(), allSegments);
-
     String datasource = task.getDataSource();
-    Map<Interval, TaskLock> replaceLocks = new HashMap<>();
-    for (TaskLock lock : TaskLocks.findReplaceLocksForSegments(datasource, toolbox.getTaskLockbox(), segments)) {
-      replaceLocks.put(lock.getInterval(), lock);
-    }
-    Set<TaskLockInfo> taskLockInfos = new HashSet<>();
-    for (TaskLock taskLock : replaceLocks.values()) {
-      taskLockInfos.add(getTaskLockInfo(taskLock));
-    }
+    final Map<DataSegment, TaskLockInfo> segmentToReplaceLock
+        = TaskLocks.findReplaceLocksCoveringSegments(datasource, toolbox.getTaskLockbox(), segments);
 
+    final SegmentPublishResult retVal;
     try {
       retVal = toolbox.getTaskLockbox().doInCriticalSection(
           task,
-          allSegments.stream().map(DataSegment::getInterval).collect(Collectors.toSet()),
+          segments.stream().map(DataSegment::getInterval).collect(Collectors.toSet()),
           CriticalAction.<SegmentPublishResult>builder()
               .onValidLocks(
                   () -> toolbox.getIndexerMetadataStorageCoordinator().commitReplaceSegments(
                       segments,
                       segmentsToBeDropped,
-                      taskLockInfos
+                      new HashSet<>(segmentToReplaceLock.values())
                   )
               )
               .onInvalidLocks(
@@ -166,28 +152,20 @@ public class SegmentTransactionalReplaceAction implements TaskAction<SegmentPubl
 
     if (retVal.isSuccess()) {
       toolbox.getEmitter().emit(metricBuilder.build("segment/txn/success", 1));
+
+      for (DataSegment segment : retVal.getSegments()) {
+        final String partitionType = segment.getShardSpec() == null ? null : segment.getShardSpec().getType();
+        metricBuilder.setDimension(DruidMetrics.PARTITIONING_TYPE, partitionType);
+        metricBuilder.setDimension(DruidMetrics.INTERVAL, segment.getInterval().toString());
+        toolbox.getEmitter().emit(metricBuilder.build("segment/added/bytes", segment.getSize()));
+      }
     } else {
       toolbox.getEmitter().emit(metricBuilder.build("segment/txn/failure", 1));
     }
 
-    // getSegments() should return an empty set if announceHistoricalSegments() failed
-    for (DataSegment segment : retVal.getSegments()) {
-      metricBuilder.setDimension(DruidMetrics.INTERVAL, segment.getInterval().toString());
-      metricBuilder.setDimension(
-          DruidMetrics.PARTITIONING_TYPE,
-          segment.getShardSpec() == null ? null : segment.getShardSpec().getType()
-      );
-      toolbox.getEmitter().emit(metricBuilder.build("segment/added/bytes", segment.getSize()));
-    }
-
     return retVal;
   }
 
-  private TaskLockInfo getTaskLockInfo(TaskLock taskLock)
-  {
-    return new TaskLockInfo(taskLock.getInterval(), taskLock.getVersion());
-  }
-
   @Override
   public boolean isAudited()
   {
diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/common/actions/TaskLocks.java b/indexing-service/src/main/java/org/apache/druid/indexing/common/actions/TaskLocks.java
index 400d0f38fd..3b4fa067bc 100644
--- a/indexing-service/src/main/java/org/apache/druid/indexing/common/actions/TaskLocks.java
+++ b/indexing-service/src/main/java/org/apache/druid/indexing/common/actions/TaskLocks.java
@@ -26,20 +26,24 @@ import org.apache.druid.indexing.common.TaskLock;
 import org.apache.druid.indexing.common.TaskLockType;
 import org.apache.druid.indexing.common.TimeChunkLock;
 import org.apache.druid.indexing.common.task.Task;
+import org.apache.druid.indexing.overlord.TaskLockInfo;
 import org.apache.druid.indexing.overlord.TaskLockbox;
 import org.apache.druid.java.util.common.ISE;
 import org.apache.druid.timeline.DataSegment;
 import org.joda.time.DateTime;
+import org.joda.time.Interval;
 
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Collections;
-import java.util.HashSet;
+import java.util.HashMap;
 import java.util.List;
+import java.util.Map;
 import java.util.Map.Entry;
 import java.util.NavigableMap;
 import java.util.Set;
 import java.util.TreeMap;
+import java.util.stream.Collectors;
 
 public class TaskLocks
 {
@@ -114,26 +118,49 @@ public class TaskLocks
     );
   }
 
-  public static Set<TaskLock> findReplaceLocksForSegments(
+  /**
+   * Finds locks of type {@link TaskLockType#REPLACE} for each of the given segments
+   * that have an interval completely covering the interval of the respective segments.
+   *
+   * @return Map from segment to REPLACE lock that completely covers it. The map
+   * does not contain an entry for segments that have no covering REPLACE lock.
+   */
+  public static Map<DataSegment, TaskLockInfo> findReplaceLocksCoveringSegments(
       final String datasource,
       final TaskLockbox taskLockbox,
-      final Collection<DataSegment> segments
+      final Set<DataSegment> segments
   )
   {
-    final Set<TaskLock> found = new HashSet<>();
-    final Set<TaskLock> locks = taskLockbox.getAllReplaceLocksForDatasource(datasource);
-    segments.forEach(segment -> {
-      locks.forEach(lock -> {
-        if (lock.getGranularity() == LockGranularity.TIME_CHUNK) {
-          final TimeChunkLock timeChunkLock = (TimeChunkLock) lock;
-          if (timeChunkLock.getInterval().contains(segment.getInterval())
-              && timeChunkLock.getDataSource().equals(segment.getDataSource())) {
-            found.add(lock);
-          }
+    // Identify unique segment intervals
+    final Map<Interval, List<DataSegment>> intervalToSegments = new HashMap<>();
+    segments.forEach(
+        segment -> intervalToSegments.computeIfAbsent(
+            segment.getInterval(), interval -> new ArrayList<>()
+        ).add(segment)
+    );
+
+    final Set<TaskLockInfo> replaceLocks = taskLockbox.getAllReplaceLocksForDatasource(datasource).stream()
+                                                      .map(TaskLocks::toLockInfo)
+                                                      .collect(Collectors.toSet());
+
+    final Map<DataSegment, TaskLockInfo> segmentToReplaceLock = new HashMap<>();
+
+    intervalToSegments.forEach((interval, segmentsForInterval) -> {
+      // For each interval, find the lock that covers it, if any
+      for (TaskLockInfo lock : replaceLocks) {
+        if (lock.getInterval().contains(interval)) {
+          segmentsForInterval.forEach(s -> segmentToReplaceLock.put(s, lock));
+          return;
         }
-      });
+      }
     });
-    return found;
+
+    return segmentToReplaceLock;
+  }
+
+  public static TaskLockInfo toLockInfo(TaskLock taskLock)
+  {
+    return new TaskLockInfo(taskLock.getInterval(), taskLock.getVersion());
   }
 
   public static List<TaskLock> findLocksForSegments(
diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/common/task/IndexTaskUtils.java b/indexing-service/src/main/java/org/apache/druid/indexing/common/task/IndexTaskUtils.java
index cd7a52f772..20f7584c8e 100644
--- a/indexing-service/src/main/java/org/apache/druid/indexing/common/task/IndexTaskUtils.java
+++ b/indexing-service/src/main/java/org/apache/druid/indexing/common/task/IndexTaskUtils.java
@@ -33,6 +33,7 @@ import org.apache.druid.server.security.ForbiddenException;
 import org.apache.druid.server.security.Resource;
 import org.apache.druid.server.security.ResourceAction;
 import org.apache.druid.server.security.ResourceType;
+import org.apache.druid.timeline.DataSegment;
 import org.apache.druid.utils.CircularBuffer;
 import org.joda.time.DateTime;
 
@@ -141,4 +142,14 @@ public class IndexTaskUtils
     metricBuilder.setDimension(DruidMetrics.TASK_ID, taskStatus.getId());
     metricBuilder.setDimension(DruidMetrics.TASK_STATUS, taskStatus.getStatusCode().toString());
   }
+
+  public static void setSegmentDimensions(
+      ServiceMetricEvent.Builder metricBuilder,
+      DataSegment segment
+  )
+  {
+    final String partitionType = segment.getShardSpec() == null ? null : segment.getShardSpec().getType();
+    metricBuilder.setDimension(DruidMetrics.PARTITIONING_TYPE, partitionType);
+    metricBuilder.setDimension(DruidMetrics.INTERVAL, segment.getInterval().toString());
+  }
 }
diff --git a/indexing-service/src/test/java/org/apache/druid/indexing/common/task/AppenderatorDriverRealtimeIndexTaskTest.java b/indexing-service/src/test/java/org/apache/druid/indexing/common/task/AppenderatorDriverRealtimeIndexTaskTest.java
index db94038e07..eaeb74866f 100644
--- a/indexing-service/src/test/java/org/apache/druid/indexing/common/task/AppenderatorDriverRealtimeIndexTaskTest.java
+++ b/indexing-service/src/test/java/org/apache/druid/indexing/common/task/AppenderatorDriverRealtimeIndexTaskTest.java
@@ -1523,14 +1523,14 @@ public class AppenderatorDriverRealtimeIndexTaskTest extends InitializedNullHand
       }
 
       @Override
-      public SegmentPublishResult commitSegments(
+      public SegmentPublishResult commitSegmentsAndMetadata(
           Set<DataSegment> segments,
           Set<DataSegment> segmentsToDrop,
           DataSourceMetadata startMetadata,
           DataSourceMetadata endMetadata
       ) throws IOException
       {
-        SegmentPublishResult result = super.commitSegments(segments, segmentsToDrop, startMetadata, endMetadata);
+        SegmentPublishResult result = super.commitSegmentsAndMetadata(segments, segmentsToDrop, startMetadata, endMetadata);
 
         Assert.assertFalse(
             "Segment latch not initialized, did you forget to call expectPublishSegments?",
diff --git a/indexing-service/src/test/java/org/apache/druid/indexing/common/task/batch/parallel/SinglePhaseParallelIndexingTest.java b/indexing-service/src/test/java/org/apache/druid/indexing/common/task/batch/parallel/SinglePhaseParallelIndexingTest.java
index 950983d77b..d22769c1f7 100644
--- a/indexing-service/src/test/java/org/apache/druid/indexing/common/task/batch/parallel/SinglePhaseParallelIndexingTest.java
+++ b/indexing-service/src/test/java/org/apache/druid/indexing/common/task/batch/parallel/SinglePhaseParallelIndexingTest.java
@@ -29,7 +29,6 @@ import org.apache.druid.data.input.impl.JsonInputFormat;
 import org.apache.druid.data.input.impl.LocalInputSource;
 import org.apache.druid.indexer.TaskState;
 import org.apache.druid.indexing.common.LockGranularity;
-import org.apache.druid.indexing.common.TaskLockType;
 import org.apache.druid.indexing.common.TaskToolbox;
 import org.apache.druid.indexing.common.actions.TaskActionClient;
 import org.apache.druid.indexing.common.task.Tasks;
diff --git a/indexing-service/src/test/java/org/apache/druid/indexing/test/TestIndexerMetadataStorageCoordinator.java b/indexing-service/src/test/java/org/apache/druid/indexing/test/TestIndexerMetadataStorageCoordinator.java
index e58df94ef8..16c775b01b 100644
--- a/indexing-service/src/test/java/org/apache/druid/indexing/test/TestIndexerMetadataStorageCoordinator.java
+++ b/indexing-service/src/test/java/org/apache/druid/indexing/test/TestIndexerMetadataStorageCoordinator.java
@@ -173,8 +173,7 @@ public class TestIndexerMetadataStorageCoordinator implements IndexerMetadataSto
       Set<DataSegment> segments,
       DataSourceMetadata oldCommitMetadata,
       DataSourceMetadata newCommitMetadata,
-      @Nullable Map<DataSegment, TaskLockInfo> segmentLockMap,
-      @Nullable Set<TaskLockInfo> taskLockInfos
+      @Nullable Map<DataSegment, TaskLockInfo> segmentLockMap
   )
   {
     // Don't actually compare metadata, just do it!
@@ -182,7 +181,7 @@ public class TestIndexerMetadataStorageCoordinator implements IndexerMetadataSto
   }
 
   @Override
-  public SegmentPublishResult commitSegments(
+  public SegmentPublishResult commitSegmentsAndMetadata(
       Set<DataSegment> segments,
       Set<DataSegment> segmentsToDrop,
       @Nullable DataSourceMetadata startMetadata,
diff --git a/server/src/main/java/org/apache/druid/indexing/overlord/DataSourceMetadata.java b/server/src/main/java/org/apache/druid/indexing/overlord/DataSourceMetadata.java
index a6955ece8b..26fc4cc9db 100644
--- a/server/src/main/java/org/apache/druid/indexing/overlord/DataSourceMetadata.java
+++ b/server/src/main/java/org/apache/druid/indexing/overlord/DataSourceMetadata.java
@@ -26,7 +26,7 @@ import java.util.Set;
 
 /**
  * Commit metadata for a dataSource. Used by
- * {@link IndexerMetadataStorageCoordinator#commitSegments(Set, Set, DataSourceMetadata, DataSourceMetadata)}
+ * {@link IndexerMetadataStorageCoordinator#commitSegmentsAndMetadata(Set, Set, DataSourceMetadata, DataSourceMetadata)}
  * to provide metadata transactions for segment inserts.
  *
  * Two metadata instances can be added together, and any conflicts are resolved in favor of the right-hand side.
diff --git a/server/src/main/java/org/apache/druid/indexing/overlord/IndexerMetadataStorageCoordinator.java b/server/src/main/java/org/apache/druid/indexing/overlord/IndexerMetadataStorageCoordinator.java
index e5e09269b9..0edadd9605 100644
--- a/server/src/main/java/org/apache/druid/indexing/overlord/IndexerMetadataStorageCoordinator.java
+++ b/server/src/main/java/org/apache/druid/indexing/overlord/IndexerMetadataStorageCoordinator.java
@@ -254,7 +254,7 @@ public interface IndexerMetadataStorageCoordinator
    * commit metadata.
    *
    * If segmentsToDrop is not null and not empty, this insertion will be atomic with a insert-and-drop on inserting
-   * {@param segments} and dropping {@param segmentsToDrop}
+   * {@param segments} and dropping {@param segmentsToDrop}.
    *
    * @param segments       set of segments to add, must all be from the same dataSource
    * @param segmentsToDrop set of segments to drop, must all be from the same dataSource
@@ -272,7 +272,7 @@ public interface IndexerMetadataStorageCoordinator
    * @throws IllegalArgumentException if startMetadata and endMetadata are not either both null or both non-null
    * @throws RuntimeException         if the state of metadata storage after this call is unknown
    */
-  SegmentPublishResult commitSegments(
+  SegmentPublishResult commitSegmentsAndMetadata(
       Set<DataSegment> segments,
       Set<DataSegment> segmentsToDrop,
       @Nullable DataSourceMetadata startMetadata,
@@ -283,8 +283,7 @@ public interface IndexerMetadataStorageCoordinator
       Set<DataSegment> segments,
       @Nullable DataSourceMetadata startMetadata,
       @Nullable DataSourceMetadata endMetadata,
-      @Nullable Map<DataSegment, TaskLockInfo> segmentLockMap,
-      @Nullable Set<TaskLockInfo> taskLockInfos
+      @Nullable Map<DataSegment, TaskLockInfo> segmentLockMap
   ) throws IOException;
 
   SegmentPublishResult commitReplaceSegments(
diff --git a/server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java b/server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java
index 5ee437914a..5e26bc33b6 100644
--- a/server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java
+++ b/server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java
@@ -309,7 +309,7 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
   @Override
   public Set<DataSegment> commitSegments(final Set<DataSegment> segments) throws IOException
   {
-    final SegmentPublishResult result = commitSegments(segments, null, null, null);
+    final SegmentPublishResult result = commitSegmentsAndMetadata(segments, null, null, null);
 
     // Metadata transaction cannot fail because we are not trying to do one.
     if (!result.isSuccess()) {
@@ -320,7 +320,7 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
   }
 
   @Override
-  public SegmentPublishResult commitSegments(
+  public SegmentPublishResult commitSegmentsAndMetadata(
       final Set<DataSegment> segments,
       final Set<DataSegment> segmentsToDrop,
       @Nullable final DataSourceMetadata startMetadata,
@@ -459,41 +459,31 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
 
     try {
       return connector.retryTransaction(
-          new TransactionCallback<SegmentPublishResult>()
-          {
-            @Override
-            public SegmentPublishResult inTransaction(
-                final Handle handle,
-                final TransactionStatus transactionStatus
-            ) throws Exception
-            {
-              // Set definitelyNotUpdated back to false upon retrying.
-              definitelyNotUpdated.set(false);
-
+          (handle, transactionStatus) -> {
+            // Set definitelyNotUpdated back to false upon retrying.
+            definitelyNotUpdated.set(false);
 
-              if (segmentsToDrop != null && !segmentsToDrop.isEmpty()) {
-                final DataStoreMetadataUpdateResult result = dropSegmentsWithHandle(
-                    handle,
-                    segmentsToDrop,
-                    dataSource
-                );
-                if (result.isFailed()) {
-                  // Metadata store was definitely not updated.
-                  transactionStatus.setRollbackOnly();
-                  definitelyNotUpdated.set(true);
+            if (segmentsToDrop != null && !segmentsToDrop.isEmpty()) {
+              final DataStoreMetadataUpdateResult result = dropSegmentsWithHandle(
+                  handle,
+                  segmentsToDrop,
+                  dataSource
+              );
+              if (result.isFailed()) {
+                // Metadata store was definitely not updated.
+                transactionStatus.setRollbackOnly();
+                definitelyNotUpdated.set(true);
 
-                  if (result.canRetry()) {
-                    throw new RetryTransactionException(result.getErrorMsg());
-                  } else {
-                    throw new RuntimeException(result.getErrorMsg());
-                  }
+                if (result.canRetry()) {
+                  throw new RetryTransactionException(result.getErrorMsg());
+                } else {
+                  throw new RuntimeException(result.getErrorMsg());
                 }
               }
-
-              final Set<DataSegment> inserted = commitReplaceSegmentBatch(handle, newSegments, usedSegments, taskLockInfos);
-
-              return SegmentPublishResult.ok(ImmutableSet.copyOf(inserted));
             }
+
+            final Set<DataSegment> inserted = commitReplaceSegmentBatch(handle, newSegments, usedSegments, taskLockInfos);
+            return SegmentPublishResult.ok(ImmutableSet.copyOf(inserted));
           },
           3,
           getSqlMetadataMaxRetry()
@@ -514,43 +504,28 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
       final Set<DataSegment> segments,
       @Nullable final DataSourceMetadata startMetadata,
       @Nullable DataSourceMetadata endMetadata,
-      @Nullable Map<DataSegment, TaskLockInfo> segmentLockMap,
-      @Nullable Set<TaskLockInfo> taskLockInfos
+      @Nullable Map<DataSegment, TaskLockInfo> segmentLockMap
   )
   {
     if (segments.isEmpty()) {
-      throw new IllegalArgumentException("segment set must not be empty");
+      throw new IllegalArgumentException("No segments to append");
     }
 
     final String dataSource = segments.iterator().next().getDataSource();
     for (DataSegment segment : segments) {
       if (!dataSource.equals(segment.getDataSource())) {
-        throw new IllegalArgumentException("segments must all be from the same dataSource");
+        throw new IllegalArgumentException("All segments to append must belong to the same dataSource");
       }
     }
 
     if ((startMetadata == null && endMetadata != null) || (startMetadata != null && endMetadata == null)) {
-      throw new IllegalArgumentException("start/end metadata pair must be either null or non-null");
+      throw new IllegalArgumentException("Start and end metadata must either be both null or both non-null");
     }
 
     // Find which segments are used (i.e. not overshadowed).
     Set<DataSegment> newSegments = new HashSet<>(segments);
     final Map<DataSegment, Set<SegmentIdWithShardSpec>> segmentToNewMetadataMap = connector.retryTransaction(
-        new TransactionCallback<Map<DataSegment, Set<SegmentIdWithShardSpec>>>()
-        {
-          @Override
-          public Map<DataSegment, Set<SegmentIdWithShardSpec>> inTransaction(
-              final Handle handle,
-              final TransactionStatus transactionStatus
-          ) throws Exception
-          {
-            return allocateNewSegmentIds(
-                handle,
-                dataSource,
-                segments
-            );
-          }
-        },
+        (handle, transactionStatus) -> allocateNewSegmentIds(handle, dataSource, segments),
         0,
         SQLMetadataConnector.DEFAULT_MAX_TRIES
     );
@@ -572,49 +547,36 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
     }
     Set<DataSegment> usedSegments = new HashSet<>(newSegments);
 
-
-
     final AtomicBoolean definitelyNotUpdated = new AtomicBoolean(false);
-
     try {
       return connector.retryTransaction(
-          new TransactionCallback<SegmentPublishResult>()
-          {
-            @Override
-            public SegmentPublishResult inTransaction(
-                final Handle handle,
-                final TransactionStatus transactionStatus
-            ) throws Exception
-            {
-              // Set definitelyNotUpdated back to false upon retrying.
-              definitelyNotUpdated.set(false);
+          (handle, transactionStatus) -> {
+            // Set definitelyNotUpdated back to false upon retrying.
+            definitelyNotUpdated.set(false);
 
-              if (startMetadata != null) {
-                final DataStoreMetadataUpdateResult result = updateDataSourceMetadataWithHandle(
-                    handle,
-                    dataSource,
-                    startMetadata,
-                    endMetadata
-                );
+            if (startMetadata != null) {
+              final DataStoreMetadataUpdateResult result = updateDataSourceMetadataWithHandle(
+                  handle,
+                  dataSource,
+                  startMetadata,
+                  endMetadata
+              );
 
-                if (result.isFailed()) {
-                  // Metadata was definitely not updated.
-                  transactionStatus.setRollbackOnly();
-                  definitelyNotUpdated.set(true);
+              if (result.isFailed()) {
+                // Metadata was definitely not updated.
+                transactionStatus.setRollbackOnly();
+                definitelyNotUpdated.set(true);
 
-                  if (result.canRetry()) {
-                    throw new RetryTransactionException(result.getErrorMsg());
-                  } else {
-                    throw new RuntimeException(result.getErrorMsg());
-                  }
+                if (result.canRetry()) {
+                  throw new RetryTransactionException(result.getErrorMsg());
+                } else {
+                  throw new RuntimeException(result.getErrorMsg());
                 }
               }
-
-
-              final Set<DataSegment> inserted = commitAppendSegmentBatch(handle, newSegments, usedSegments, segmentLockMap);
-
-              return SegmentPublishResult.ok(ImmutableSet.copyOf(inserted));
             }
+
+            final Set<DataSegment> inserted = commitAppendSegmentBatch(handle, newSegments, usedSegments, segmentLockMap);
+            return SegmentPublishResult.ok(ImmutableSet.copyOf(inserted));
           },
           3,
           getSqlMetadataMaxRetry()
@@ -1299,7 +1261,7 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
         }
         for (DataSegment segment : intervalToSegments.get(interval)) {
           SegmentCreateRequest request = new SegmentCreateRequest(
-              segment.getId().toString() + version,
+              segment.getId() + version,
               null,
               version,
               NumberedPartialShardSpec.instance()
@@ -1867,7 +1829,7 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
         for (Map.Entry<String, TaskLockInfo> entry : batch) {
           batchMap.put(entry.getKey(), entry.getValue());
         }
-        List<DataSegment> oldSegments = retrieveSegments(handle, batchMap.keySet());
+        List<DataSegment> oldSegments = retrieveSegmentsById(handle, batchMap.keySet());
         for (DataSegment oldSegment : oldSegments) {
           Interval newInterval = oldSegment.getInterval();
           for (DataSegment segment : segments) {
@@ -2007,122 +1969,7 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
           DataSegment segment = entry.getKey();
           TaskLockInfo lock = entry.getValue();
           appendBatch.add()
-                     .bind("id", segment.getId().toString() + ":" + lock.hashCode())
-                     .bind("dataSource", segment.getDataSource())
-                     .bind("start", lock.getInterval().getStartMillis())
-                     .bind("end", lock.getInterval().getEndMillis())
-                     .bind("segment_id", segment.getId().toString())
-                     .bind("lock_version", lock.getVersion());
-        }
-        final int[] affectedAppendRows = appendBatch.execute();
-        final boolean succeeded = Arrays.stream(affectedAppendRows).allMatch(eachAffectedRow -> eachAffectedRow == 1);
-        if (!succeeded) {
-          final List<DataSegment> failedToForward = IntStream.range(0, partition.size())
-                                                             .filter(i -> affectedAppendRows[i] != 1)
-                                                             .mapToObj(partition::get)
-                                                             .map(x -> x.getKey())
-                                                             .collect(Collectors.toList());
-          throw new ISE(
-              "Failed to forward appended segments to DB: %s",
-              SegmentUtils.commaSeparatedIdentifiers(failedToForward)
-          );
-        }
-      }
-    }
-    catch (Exception e) {
-      log.errorSegments(segments, "Exception inserting segment metadata");
-      throw e;
-    }
-
-    return toInsertSegments;
-  }
-
-  private Set<DataSegment> commitSegmentBatch(
-      final Handle handle,
-      final Set<DataSegment> segments,
-      final Set<DataSegment> usedSegments,
-      @Nullable Map<DataSegment, TaskLockInfo> appendSegmentLockMap,
-      @Nullable Set<TaskLockInfo> replaceLocks,
-      boolean append
-  ) throws IOException
-  {
-    final Set<DataSegment> toInsertSegments = new HashSet<>();
-    try {
-      Set<String> existedSegments = segmentExistsBatch(handle, segments);
-      log.info("Found these segments already exist in DB: %s", existedSegments);
-      for (DataSegment segment : segments) {
-        if (!existedSegments.contains(segment.getId().toString())) {
-          toInsertSegments.add(segment);
-        }
-      }
-
-      // SELECT -> INSERT can fail due to races; callers must be prepared to retry.
-      // Avoiding ON DUPLICATE KEY since it's not portable.
-      // Avoiding try/catch since it may cause inadvertent transaction-splitting.
-      final List<List<DataSegment>> partitionedSegments = Lists.partition(
-          new ArrayList<>(toInsertSegments),
-          MAX_NUM_SEGMENTS_TO_ANNOUNCE_AT_ONCE
-      );
-
-      PreparedBatch preparedBatch = handle.prepareBatch(
-          StringUtils.format(
-              "INSERT INTO %1$s (id, dataSource, created_date, start, %2$send%2$s, partitioned, version, used, payload) "
-              + "VALUES (:id, :dataSource, :created_date, :start, :end, :partitioned, :version, :used, :payload)",
-              dbTables.getSegmentsTable(),
-              connector.getQuoteString()
-          )
-      );
-      for (List<DataSegment> partition : partitionedSegments) {
-        for (DataSegment segment : partition) {
-          preparedBatch.add()
-                       .bind("id", segment.getId().toString())
-                       .bind("dataSource", segment.getDataSource())
-                       .bind("created_date", DateTimes.nowUtc().toString())
-                       .bind("start", segment.getInterval().getStart().toString())
-                       .bind("end", segment.getInterval().getEnd().toString())
-                       .bind("partitioned", (segment.getShardSpec() instanceof NoneShardSpec) ? false : true)
-                       .bind("version", segment.getVersion())
-                       .bind("used", usedSegments.contains(segment))
-                       .bind("payload", jsonMapper.writeValueAsBytes(segment));
-        }
-        final int[] affectedInsertRows = preparedBatch.execute();
-
-        final boolean succeeded = Arrays.stream(affectedInsertRows).allMatch(eachAffectedRow -> eachAffectedRow == 1);
-        if (succeeded) {
-          log.infoSegments(partition, "Published segments to DB");
-        } else {
-          final List<DataSegment> failedToPublish = IntStream.range(0, partition.size())
-                                                             .filter(i -> affectedInsertRows[i] != 1)
-                                                             .mapToObj(partition::get)
-                                                             .collect(Collectors.toList());
-          throw new ISE(
-              "Failed to publish segments to DB: %s",
-              SegmentUtils.commaSeparatedIdentifiers(failedToPublish)
-          );
-        }
-      }
-
-      PreparedBatch appendBatch = handle.prepareBatch(
-          StringUtils.format(
-              "INSERT INTO %1$s (id, dataSource, start, %2$send%2$s, segment_id, lock_version) "
-              + "VALUES (:id, :dataSource, :start, :end, :segment_id, :lock_version)",
-              dbTables.getSegmentVersionsTable(),
-              connector.getQuoteString()
-          )
-      );
-      if (appendSegmentLockMap == null) {
-        appendSegmentLockMap = new HashMap<>();
-      }
-      final List<List<Map.Entry<DataSegment, TaskLockInfo>>> appendSegmentPartitions = Lists.partition(
-          new ArrayList<>(appendSegmentLockMap.entrySet()),
-          MAX_NUM_SEGMENTS_TO_ANNOUNCE_AT_ONCE
-      );
-      for (List<Map.Entry<DataSegment, TaskLockInfo>> partition : appendSegmentPartitions) {
-        for (Map.Entry<DataSegment, TaskLockInfo> entry : partition) {
-          DataSegment segment = entry.getKey();
-          TaskLockInfo lock = entry.getValue();
-          appendBatch.add()
-                     .bind("id", segment.getId().toString() + ":" + lock.hashCode())
+                     .bind("id", segment.getId() + ":" + lock.hashCode())
                      .bind("dataSource", segment.getDataSource())
                      .bind("start", lock.getInterval().getStartMillis())
                      .bind("end", lock.getInterval().getEndMillis())
@@ -2143,74 +1990,6 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
           );
         }
       }
-
-      Map<String, TaskLockInfo> segmentsToBeForwarded = new HashMap<>();
-      if (!append) {
-        segmentsToBeForwarded = getAppendedSegmentIds(
-            handle,
-            segments.iterator().next().getDataSource(),
-            replaceLocks
-        );
-      }
-      final int numCorePartitions = segments.size();
-      int partitionNum = segments.size();
-      final List<List<Map.Entry<String, TaskLockInfo>>> forwardSegmentsBatch = Lists.partition(
-          new ArrayList<>(segmentsToBeForwarded.entrySet()),
-          MAX_NUM_SEGMENTS_TO_ANNOUNCE_AT_ONCE
-      );
-      for (List<Map.Entry<String, TaskLockInfo>> batch : forwardSegmentsBatch) {
-        Map<String, TaskLockInfo> batchMap = new HashMap<>();
-        for (Map.Entry<String, TaskLockInfo> entry : batch) {
-          batchMap.put(entry.getKey(), entry.getValue());
-        }
-        List<DataSegment> oldSegments = retrieveSegments(handle, batchMap.keySet());
-        for (DataSegment oldSegment : oldSegments) {
-          Interval newInterval = oldSegment.getInterval();
-          for (DataSegment segment : segments) {
-            if (segment.getInterval().overlaps(newInterval)) {
-              if (segment.getInterval().contains(newInterval)) {
-                newInterval = segment.getInterval();
-              } else {
-                throw new ISE("Incompatible segment intervals for commit: [%s] and [%s].",
-                              newInterval,
-                              segment.getInterval()
-                );
-              }
-            }
-          }
-          TaskLockInfo lock = batchMap.get(oldSegment.getId().toString());
-          ShardSpec shardSpec = new NumberedShardSpec(partitionNum++, numCorePartitions);
-          DataSegment newSegment = new DataSegment(
-              oldSegment.getDataSource(),
-              newInterval,
-              lock.getVersion(),
-              oldSegment.getLoadSpec(),
-              oldSegment.getDimensions(),
-              oldSegment.getMetrics(),
-              shardSpec,
-              oldSegment.getBinaryVersion(),
-              oldSegment.getSize()
-          );
-          preparedBatch.add()
-                       .bind("id", newSegment.getId().toString())
-                       .bind("dataSource", newSegment.getDataSource())
-                       .bind("created_date", DateTimes.nowUtc().toString())
-                       .bind("start", newSegment.getInterval().getStart().toString())
-                       .bind("end", newSegment.getInterval().getEnd().toString())
-                       .bind("partitioned", (newSegment.getShardSpec() instanceof NoneShardSpec) ? false : true)
-                       .bind("version", newSegment.getVersion())
-                       .bind("used", true)
-                       .bind("payload", jsonMapper.writeValueAsBytes(newSegment));
-        }
-        final int[] affectedInsertRows = preparedBatch.execute();
-
-        final boolean succeeded = Arrays.stream(affectedInsertRows).allMatch(eachAffectedRow -> eachAffectedRow == 1);
-        if (succeeded) {
-          log.info("Published segments with updated metadata to DB");
-        } else {
-          throw new ISE("Failed to update segment metadatas in DB");
-        }
-      }
     }
     catch (Exception e) {
       log.errorSegments(segments, "Exception inserting segment metadata");
@@ -2220,37 +1999,22 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
     return toInsertSegments;
   }
 
-  private List<DataSegment> retrieveSegments(final Handle handle, final Set<String> segmentIds)
+  private List<DataSegment> retrieveSegmentsById(Handle handle, Set<String> segmentIds)
   {
-    final StringBuilder sb = new StringBuilder();
-    sb.append("SELECT payload FROM %s WHERE id in (");
-
-    List<String> segmentIdList = new ArrayList<>(segmentIds);
-    int n = segmentIdList.size();
-    for (int i = 0; i < n; i++) {
-      sb.append("'");
-      sb.append(segmentIdList.get(i));
-      sb.append("'");
-      if (i < n - 1) {
-        sb.append(", ");
-      }
-    }
-
-    sb.append(")");
-
-    final Query<Map<String, Object>> sql = handle
-        .createQuery(StringUtils.format(sb.toString(), dbTables.getSegmentsTable()))
-        .setFetchSize(connector.getStreamingFetchSize());
+    final String segmentIdCsv = segmentIds.stream().map(id -> "'" + id + "'")
+                                          .collect(Collectors.joining(","));
+    final Query<Map<String, Object>> query = handle.createQuery(
+        StringUtils.format(
+            "SELECT payload FROM %s WHERE id in (%s)",
+            segmentIdCsv, dbTables.getSegmentsTable()
+        )
+    ).setFetchSize(connector.getStreamingFetchSize());
 
-    final ResultIterator<DataSegment> resultIterator =
-        sql.map((index, r, ctx) -> JacksonUtils.readValue(jsonMapper, r.getBytes(1), DataSegment.class))
-           .iterator();
+    ResultIterator<DataSegment> resultIterator = query.map(
+        (index, r, ctx) -> JacksonUtils.readValue(jsonMapper, r.getBytes(1), DataSegment.class)
+    ).iterator();
 
-    List<DataSegment> retVal = new ArrayList<>();
-    while (resultIterator.hasNext()) {
-      retVal.add(resultIterator.next());
-    }
-    return retVal;
+    return Lists.newArrayList(resultIterator);
   }
 
   private Map<String, TaskLockInfo> getAppendedSegmentIds(
@@ -2383,7 +2147,7 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
    *
    * @return SUCCESS if dataSource metadata was updated from matching startMetadata to matching endMetadata, FAILURE or
    * TRY_AGAIN if it definitely was not updated. This guarantee is meant to help
-   * {@link #commitSegments(Set, Set, DataSourceMetadata, DataSourceMetadata)}
+   * {@link #commitSegmentsAndMetadata(Set, Set, DataSourceMetadata, DataSourceMetadata)}
    * achieve its own guarantee.
    *
    * @throws RuntimeException if state is unknown after this call
@@ -2513,7 +2277,7 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
    *
    * @return SUCCESS if segment was marked unused, FAILURE or
    * TRY_AGAIN if it definitely was not updated. This guarantee is meant to help
-   * {@link #commitSegments(Set, Set, DataSourceMetadata, DataSourceMetadata)}
+   * {@link #commitSegmentsAndMetadata(Set, Set, DataSourceMetadata, DataSourceMetadata)}
    * achieve its own guarantee.
    *
    * @throws RuntimeException if state is unknown after this call
diff --git a/server/src/main/java/org/apache/druid/metadata/SQLMetadataConnector.java b/server/src/main/java/org/apache/druid/metadata/SQLMetadataConnector.java
index a802c3c655..669ffefae2 100644
--- a/server/src/main/java/org/apache/druid/metadata/SQLMetadataConnector.java
+++ b/server/src/main/java/org/apache/druid/metadata/SQLMetadataConnector.java
@@ -346,7 +346,7 @@ public abstract class SQLMetadataConnector implements MetadataStorageConnector
     );
   }
 
-  public void createSegmentVersionTable(final String tableName)
+  private void createSegmentVersionTable(final String tableName)
   {
     createTable(
         tableName,
diff --git a/server/src/test/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinatorTest.java b/server/src/test/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinatorTest.java
index eb55f04e20..8845a3beef 100644
--- a/server/src/test/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinatorTest.java
+++ b/server/src/test/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinatorTest.java
@@ -558,7 +558,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest
   public void testTransactionalAnnounceSuccess() throws IOException
   {
     // Insert first segment.
-    final SegmentPublishResult result1 = coordinator.commitSegments(
+    final SegmentPublishResult result1 = coordinator.commitSegmentsAndMetadata(
         ImmutableSet.of(defaultSegment),
         ImmutableSet.of(),
         new ObjectMetadata(null),
@@ -577,7 +577,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest
     );
 
     // Insert second segment.
-    final SegmentPublishResult result2 = coordinator.commitSegments(
+    final SegmentPublishResult result2 = coordinator.commitSegmentsAndMetadata(
         ImmutableSet.of(defaultSegment2),
         ImmutableSet.of(),
         new ObjectMetadata(ImmutableMap.of("foo", "bar")),
@@ -634,7 +634,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest
     };
 
     // Insert first segment.
-    final SegmentPublishResult result1 = failOnceCoordinator.commitSegments(
+    final SegmentPublishResult result1 = failOnceCoordinator.commitSegmentsAndMetadata(
         ImmutableSet.of(defaultSegment),
         ImmutableSet.of(),
         new ObjectMetadata(null),
@@ -656,7 +656,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest
     attemptCounter.set(0);
 
     // Insert second segment.
-    final SegmentPublishResult result2 = failOnceCoordinator.commitSegments(
+    final SegmentPublishResult result2 = failOnceCoordinator.commitSegmentsAndMetadata(
         ImmutableSet.of(defaultSegment2),
         ImmutableSet.of(),
         new ObjectMetadata(ImmutableMap.of("foo", "bar")),
@@ -687,7 +687,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest
   @Test
   public void testTransactionalAnnounceFailDbNullWantNotNull() throws IOException
   {
-    final SegmentPublishResult result1 = coordinator.commitSegments(
+    final SegmentPublishResult result1 = coordinator.commitSegmentsAndMetadata(
         ImmutableSet.of(defaultSegment),
         ImmutableSet.of(),
         new ObjectMetadata(ImmutableMap.of("foo", "bar")),
@@ -721,7 +721,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest
                                             .build();
     Set<DataSegment> dropSegments = ImmutableSet.of(existingSegment1, existingSegment2, dataSegmentBar);
 
-    final SegmentPublishResult result1 = coordinator.commitSegments(
+    final SegmentPublishResult result1 = coordinator.commitSegmentsAndMetadata(
         SEGMENTS,
         dropSegments,
         null,
@@ -750,7 +750,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest
         retrieveUsedSegmentIds()
     );
 
-    final SegmentPublishResult result1 = coordinator.commitSegments(
+    final SegmentPublishResult result1 = coordinator.commitSegmentsAndMetadata(
         SEGMENTS,
         ImmutableSet.of(existingSegment1, existingSegment2),
         null,
@@ -788,7 +788,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest
     );
 
     Set<DataSegment> dropSegments = ImmutableSet.of(existingSegment1, defaultSegment4);
-    final SegmentPublishResult result1 = coordinator.commitSegments(
+    final SegmentPublishResult result1 = coordinator.commitSegmentsAndMetadata(
         SEGMENTS,
         dropSegments,
         null,
@@ -809,7 +809,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest
   @Test
   public void testTransactionalAnnounceFailDbNotNullWantNull() throws IOException
   {
-    final SegmentPublishResult result1 = coordinator.commitSegments(
+    final SegmentPublishResult result1 = coordinator.commitSegmentsAndMetadata(
         ImmutableSet.of(defaultSegment),
         ImmutableSet.of(),
         new ObjectMetadata(null),
@@ -817,7 +817,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest
     );
     Assert.assertEquals(SegmentPublishResult.ok(ImmutableSet.of(defaultSegment)), result1);
 
-    final SegmentPublishResult result2 = coordinator.commitSegments(
+    final SegmentPublishResult result2 = coordinator.commitSegmentsAndMetadata(
         ImmutableSet.of(defaultSegment2),
         ImmutableSet.of(),
         new ObjectMetadata(null),
@@ -835,7 +835,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest
   @Test
   public void testTransactionalAnnounceFailDbNotNullWantDifferent() throws IOException
   {
-    final SegmentPublishResult result1 = coordinator.commitSegments(
+    final SegmentPublishResult result1 = coordinator.commitSegmentsAndMetadata(
         ImmutableSet.of(defaultSegment),
         ImmutableSet.of(),
         new ObjectMetadata(null),
@@ -843,7 +843,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest
     );
     Assert.assertEquals(SegmentPublishResult.ok(ImmutableSet.of(defaultSegment)), result1);
 
-    final SegmentPublishResult result2 = coordinator.commitSegments(
+    final SegmentPublishResult result2 = coordinator.commitSegmentsAndMetadata(
         ImmutableSet.of(defaultSegment2),
         ImmutableSet.of(),
         new ObjectMetadata(ImmutableMap.of("foo", "qux")),
@@ -1391,7 +1391,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest
   @Test
   public void testDeleteDataSourceMetadata() throws IOException
   {
-    coordinator.commitSegments(
+    coordinator.commitSegmentsAndMetadata(
         ImmutableSet.of(defaultSegment),
         ImmutableSet.of(),
         new ObjectMetadata(null),
@@ -2347,7 +2347,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest
   @Test
   public void testRemoveDataSourceMetadataOlderThanDatasourceActiveShouldNotBeDeleted() throws Exception
   {
-    coordinator.commitSegments(
+    coordinator.commitSegmentsAndMetadata(
         ImmutableSet.of(defaultSegment),
         ImmutableSet.of(),
         new ObjectMetadata(null),
@@ -2376,7 +2376,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest
   @Test
   public void testRemoveDataSourceMetadataOlderThanDatasourceNotActiveAndOlderThanTimeShouldBeDeleted() throws Exception
   {
-    coordinator.commitSegments(
+    coordinator.commitSegmentsAndMetadata(
         ImmutableSet.of(defaultSegment),
         ImmutableSet.of(),
         new ObjectMetadata(null),
@@ -2402,7 +2402,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest
   public void testRemoveDataSourceMetadataOlderThanDatasourceNotActiveButNotOlderThanTimeShouldNotBeDeleted()
       throws Exception
   {
-    coordinator.commitSegments(
+    coordinator.commitSegmentsAndMetadata(
         ImmutableSet.of(defaultSegment),
         ImmutableSet.of(),
         new ObjectMetadata(null),
